<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jungho Lee</title>
  
  <meta name="author" content="Jungho Lee">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jungho Lee</name>
              </p>
              <p>
                I am Ph.D candidate at <a href="https://www.yonsei.ac.kr">Yonsei University</a> in <a href="https://en.wikipedia.org/wiki/Seoul">Seoul</a>, where I work on computer vision and machine learning.
              </p>
              <p>
                My research mainly focuses on various 3D computer vision tasks using images and point cloud. Recent research field is related to neural rendering and its applications. 
              </p>
              <p>
                I'm always open to collaborations or suggestions. Please feel free to contact me if you have any questions or suggestions. :)
              </p>
              <!-- <p>
                I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://www.matthewtancik.com/nerf">NeRF</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
              </p> -->
              <p style="text-align:center">
                <a href="mailto:2015142131@yonsei.ac.kr">Email</a> &nbsp/&nbsp
                <a href="data/resume/CV_Jungho_Lee.pdf">CV</a> &nbsp/&nbsp
                <!-- <a href="data/resume/dogyoonlee-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.co.kr/citations?user=NAj3cTcAAAAJ&hl=ko">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/Jho-Yonsei/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile/JunghoLee.png">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publication</heading>
              <!-- <p> -->
                <!-- I'm interested in 2D/3D computer vision, graphics, and machine learning. Recent research topic is mainly related to neural rendering with various conditions. Representative papers are <span class="highlight">highlighted</span>. -->
              <!-- </p> -->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				
          
<tr onmouseout="hdgcn_stop()" onmouseover="hdgcn_start()">
  <td style="padding:35px;width:28%;vertical-align:middle">
    <div class="one">
      <div class="two" id='hdgcn_image'>
        <img src='images/hdgcn/hdgcn_after.png' width="240"></div>
      <img src='images/hdgcn/hdgcn_before.png' width="240">
    </div>
    <script type="text/javascript">
      function hdgcn_start() {
        document.getElementById('hdgcn_image').style.opacity = "1";
      }

      function hdgcn_stop() {
        document.getElementById('hdgcn_image').style.opacity = "0";
      }
      hdgcn_stop()
    </script>
  </td>
  <td style="padding:35px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2208.10741.pdf">
      <papertitle>Hierarchically Decomposed Graph Convolutional Networks for Skeleton- Based Action Recognition</papertitle>
    </a>
    <br>
    <strong>Jungho Lee</strong>,
    <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
    <a href="https://dogyoonlee.github.io/">Dogyoon Lee</a>,
    <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
    <br>
    <em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2023 
    <br>
    <a href="https://arxiv.org/pdf/2208.10741.pdf">Paper</a>
    /
    <a href="https://github.com/Jho-Yonsei/HD-GCN">Code</a>
    /
    <a href="data/bib/hdgcn2023.txt">bib</a>
    <p></p>
    <p>
    We propose a hierarchically decomposed graph convolution with a novel hierarchically decomposed graph, which consider the sematic correlation between the joints and the edges of the human skeleton.
    </p>
  </td>
</tr>

<tr onmouseout="stcnet_stop()" onmouseover="stcnet_start()">
  <td style="padding:35px;width:28%;vertical-align:middle">
    <div class="one">
      <div class="two" id='stcnet_image'>
        <img src='images/stcnet/stcnet_after.png' width="240"></div>
      <img src='images/stcnet/stcnet_before.png' width="240">
    </div>
    <script type="text/javascript">
      function stcnet_start() {
        document.getElementById('stcnet_image').style.opacity = "1";
      }

      function stcnet_stop() {
        document.getElementById('stcnet_image').style.opacity = "0";
      }
      stcnet_stop()
    </script>
  </td>
  <td style="padding:35px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2212.04761.pdf">
      <papertitle>Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</papertitle>
    </a>
    <br>
    <strong>Jungho Lee</strong>,
    <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
    <a href="https://suhwan-cho.github.io/">Suhwan Cho</a>,
    <a href="https://scholar.google.com/citations?user=q1BRGh0AAAAJ&hl=ko">Sungmin Woo</a>,
    <a href="https://scholar.google.com/citations?user=93X9IUsAAAAJ&hl=ko">Sungjun Jang</a>,
    <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
    <br>
    <em>IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2023 
    <br>
    <a href="https://arxiv.org/pdf/2212.04761.pdf">Paper</a>
    /
    <a href="https://github.com/Jho-Yonsei/STC-Net">Code</a>
    /
    <a href="data/bib/hdgcn2023.txt">bib</a>
    <p></p>
    <p>
    We propose a hierarchically decomposed graph convolution with a novel hierarchically decomposed graph, which consider the sematic correlation between the joints and the edges of the human skeleton.
    </p>
  </td>
</tr>

</tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Pending</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					



          <tr onmouseout="glcod_stop()" onmouseover="glcod_start()">
            <td style="padding:35px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='gl_cod_image'>
                  <img src='images/gl_cod/gl_cod_after.png' width="240"></div>
                <img src='images/gl_cod/gl_cod_before.png' width="240">
              </div>
              <script type="text/javascript">
                function glcod_start() {
                  document.getElementById('gl_cod_image').style.opacity = "1";
                }
          
                function glcod_stop() {
                  document.getElementById('gl_cod_image').style.opacity = "0";
                }
                glcod_stop()
              </script>
            </td>
            <td style="padding:35px;width:75%;vertical-align:middle">
              <!-- <a href="https://dreamfusion3d.github.io/"> -->
              <a href="https://arxiv.org/pdf/2211.12048.pdf">
                <papertitle>Global-Local Aggregation with Deformable Point Sampling for Camouflaged Object Detection</papertitle>
              </a>
              <br>
              <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
              <a href="https://suhwan-cho.github.io">Suhwan Cho</a>,
              <a href="https://scholar.google.com/citations?user=OskGL0sAAAAJ&hl=ko&oi=ao">Chaewon Park</a>,
              <a href="https://dogyoonlee.github.io">Dogyoon Lee</a>,
              <strong>Jungho Lee</strong>,
              <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
              <br>
              <!-- <em>Preprint</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
              <em>Pending</em>, 2023 
              <br>
              <a href="https://arxiv.org/pdf/2211.12048.pdf">Paper</a>
              /
              <a href="https://hydragon.co.kr/">Code</a>
              /
              <a href="data/bib/gl_cod2023.txt">bib</a>
              <!-- /
              <a href="https://dreamfusion3d.github.io/gallery.html">gallery</a> -->
              <p></p>
              <p>
              <!-- We optimize a NeRF from scratch using a pretrained text-to-image diffusion model to do text-to-3D generative modeling. -->
              We propose novel deformable point sampling method and global-local aggregation architecture to integrate object's global information, background, and boundary local information to improve the camouflaged object detection.
              </p>
            </td>
          </tr>
          

          <tr onmouseout="gsa_stop()" onmouseover="gsa_start()">
            <td style="padding:35px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='gsa_image'>
                  <img src='images/gsa/slot_attention_after.png' width="240"></div>
                <img src='images/gsa/slot_attention_before.png' width="240">
              </div>
              <script type="text/javascript">
                function gsa_start() {
                  document.getElementById('gsa_image').style.opacity = "1";
                }
          
                function gsa_stop() {
                  document.getElementById('gsa_image').style.opacity = "0";
                }
                gsa_stop()
              </script>
            </td>
            <td style="padding:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2303.08314.pdf">
                <papertitle>Guided Slot Attention for Unsupervised Video Object Segmentation</papertitle>
              </a>
              <br>
              <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
              <a href="https://suhwan-cho.github.io">Suhwan Cho</a>,
              <a href="https://dogyoonlee.github.io/">Dogyoon Lee</a>,
              <a href="https://scholar.google.com/citations?user=OskGL0sAAAAJ&hl=ko&oi=ao">Chaewon Park</a>,
              <strong>Jungho Lee</strong>,
              <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
              <br>
              <em>Pending</em>, 2023 
              <br>
              <a href="https://arxiv.org/pdf/2303.08314.pdf">Paper</a>
              /
              <a href="https://">Code</a>
              /
              <a href="https://">bib</a>
              <p></p>
              <p>
              We propose a guided slot attention network to reinforce spatial structural information and obtain better foreground‚Äìbackground separation.
              </p>
            </td>
          </tr>
        

					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website's source code is borrowed from <a href="https://github.com/jonbarron/jonbarron_website">jonbarron's website</a>.
              </p>
              <p style="text-align:right;font-size:small;">
                Last updated July 2023.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
