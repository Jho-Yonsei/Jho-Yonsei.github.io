<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jungho Lee</title>
  
  <meta name="author" content="Jungho Lee">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="images/logo/yonsei_logo.png">
</head>

<body>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jungho Lee</name>
              </p>
              <p>
                I am a Ph.D candidate at <a href="https://www.yonsei.ac.kr">Yonsei University</a> in <a href="https://en.wikipedia.org/wiki/Seoul">Seoul</a>, where I work on computer vision and machine learning.
              </p>
              <p>
                My primary areas of research are 3D computer vision techniques including 3D Gaussian Splatting (3D-GS) and Neural Radiance Fields (NeRF), and video understanding, specifically focusing on action recognition.
              </p>
              <p>
                I'm always open to collaborations or suggestions. Please feel free to contact me if you have any questions or suggestions. :)
              </p>
              <p style="text-align:center">
                <a href="mailto:2015142131@yonsei.ac.kr">Email</a> &nbsp/&nbsp
                <a href="data/resume/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/jungho-lee-a7b772284/">Linkedin</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.kr/citations?user=NAj3cTcAAAAJ&hl=ko">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Jho-Yonsei/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/profile/JunghoLee.png">
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publication</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<tr onmouseout="hdgcn_stop()" onmouseover="hdgcn_start()" bgcolor="#ffffd0">
  <td style="padding:20px;width:30%;height:240px;vertical-align:top">
    <div class="one">
      <div class="two" id='hdgcn_image'>
        <img src='images/hdgcn/hdgcn_after.png' width="240"></div>
      <img src='images/hdgcn/hdgcn_before.png' width="240">
    </div>
    <script type="text/javascript">
      function hdgcn_start() {
        document.getElementById('hdgcn_image').style.opacity = "1";
      }

      function hdgcn_stop() {
        document.getElementById('hdgcn_image').style.opacity = "0";
      }
      hdgcn_stop()
    </script>
  </td>
  <td style="padding:20px;width:70%;height:240px;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2208.10741.pdf">
      <papertitle>Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</papertitle>
    </a>
    <br>
    <strong>Jungho Lee</strong>,
    <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
    <a href="https://dogyoonlee.github.io/">Dogyoon Lee</a>,
    <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
    <br>
    <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023 
    <br>
    [
    <a href="https://arxiv.org/pdf/2208.10741.pdf">Paper</a>
    /
    <a href="https://github.com/Jho-Yonsei/HD-GCN">Code</a>
    /
    <a href="data/bib/hdgcn2023.txt">bib</a>
    ]
    <p></p>
    <p>
    We propose a hierarchically decomposed graph convolution with a novel hierarchically decomposed graph, which consider the sematic correlation between the joints and the edges of the human skeleton.
    </p>
  </td>
</tr>

<tr onmouseout="stcnet_stop()" onmouseover="stcnet_start()" bgcolor="#ffffd0">
  <td style="padding:20px;width:30%;height:240px;vertical-align:top">
    <div class="one">
      <div class="two" id='stcnet_image'>
        <img src='images/stcnet/stcnet_after.png' width="240"></div>
      <img src='images/stcnet/stcnet_before.png' width="240">
    </div>
    <script type="text/javascript">
      function stcnet_start() {
        document.getElementById('stcnet_image').style.opacity = "1";
      }

      function stcnet_stop() {
        document.getElementById('stcnet_image').style.opacity = "0";
      }
      stcnet_stop()
    </script>
  </td>
  <td style="padding:20px;width:70%;height:240px;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2212.04761.pdf">
      <papertitle>Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</papertitle>
    </a>
    <br>
    <strong>Jungho Lee</strong>,
    <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
    <a href="https://suhwan-cho.github.io/">Suhwan Cho</a>,
    <a href="https://scholar.google.com/citations?user=q1BRGh0AAAAJ&hl=ko">Sungmin Woo</a>,
    <a href="https://scholar.google.com/citations?user=93X9IUsAAAAJ&hl=ko">Sungjun Jang</a>,
    <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
    <br>
    <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023 
    <br>
    [
    <a href="https://arxiv.org/pdf/2212.04761.pdf">Paper</a>
    /
    <a href="https://github.com/Jho-Yonsei/STC-Net">Code</a>
    /
    <a href="data/bib/stcnet2023.txt">bib</a>
    ]
    <p></p>
    <p>
      We propose a novel Spatio-Temporal Curve Network (STC-Net) for skeleton-based action recognition, which consists of spatial modules with an spatio-temporal curve (STC) module and graph convolution with dilated kernels (DK-GC)
    </p>
  </td>
</tr>


<tr onmouseout="gsa_stop()" onmouseover="gsa_start()">
  <td style="padding:20px;width:30%;height:220px;vertical-align:top">
    <div class="one">
      <div class="two" id='gsa_image'>
        <img src='images/gsa/slot_attention_after.png' width="240"></div>
      <img src='images/gsa/slot_attention_before.png' width="240">
    </div>
    <script type="text/javascript">
      function gsa_start() {
        document.getElementById('gsa_image').style.opacity = "1";
      }

      function gsa_stop() {
        document.getElementById('gsa_image').style.opacity = "0";
      }
      gsa_stop()
    </script>
  </td>
  <td style="padding:20px;width:70%;height:220px;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2303.08314.pdf">
      <papertitle>Guided Slot Attention for Unsupervised Video Object Segmentation</papertitle>
    </a>
    <br>
    <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
    <a href="https://suhwan-cho.github.io">Suhwan Cho</a>,
    <a href="https://dogyoonlee.github.io/">Dogyoon Lee</a>,
    <a href="https://scholar.google.com/citations?user=OskGL0sAAAAJ&hl=ko&oi=ao">Chaewon Park</a>,
    <strong>Jungho Lee</strong>,
    <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
    <br>
    <em>IEEE/CVF Computer Vision and Pattern Recognition Conference (<strong>CVPR</strong>)</em>, 2024
    <br>
    [
    <a href="https://arxiv.org/pdf/2303.08314.pdf">Paper</a>
    /
    <a href="https://github.com/Hydragon516/GSANet">Code</a>
    /
    <a href="data/bib/gsa2024.txt">bib</a>
    ]
    <p></p>
    <p>
    We propose a guided slot attention network to reinforce spatial structural information and obtain better foregroundâ€“background separation.
    </p>
  </td>
</tr>

<tr onmouseout="mssgcn_stop()" onmouseover="mssgcn_start()">
  <td style="padding:20px;width:30%;height:220px;vertical-align:top">
    <div class="one">
      <div class="two" id='mssgcn_image'>
        <img src='images/mssgcn/mssgcn_after.png' width="240"></div>
      <img src='images/mssgcn/mssgcn_before.png' width="240">
    </div>
    <script type="text/javascript">
      function mssgcn_start() {
        document.getElementById('mssgcn_image').style.opacity = "1";
      }

      function mssgcn_stop() {
        document.getElementById('mssgcn_image').style.opacity = "0";
      }
      mssgcn_stop()
    </script>
  </td>
  <td style="padding:20px;width:70%;height:220px;vertical-align:middle">
    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10464339">
      <papertitle>Multi-scale Structural Graph Convolutional Network for Skeleton-based Action Recognition</papertitle>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=93X9IUsAAAAJ&hl=ko">Sungjun Jang</a>,
    <a href="https://scholar.google.com/citations?user=JeUKiGcAAAAJ&hl=ko">Heansung Lee</a>,
    <a href="https://scholar.google.com/citations?user=_VnDn7gAAAAJ&hl=ko">Woo jin Kim</a>,
    <strong>Jungho Lee</strong>,
    <a href="https://scholar.google.com/citations?user=q1BRGh0AAAAJ&hl=ko">Sungmin Woo</a>,
    <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
    <br>
    <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)</em>, 2024
    <br>
    [
    <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10464339">Paper</a>
    /
    <a href="data/bib/mssgcn2024.txt">bib</a>
    ]
    <p></p>
    <p>
    We propose the multi-scale structural graph convolutional network (MSS-GCN) for skeleton-based action recognition, the common intersection graph convolution (CI-GC) leverages the overlapped neighbor information between neighboring vertices for a given pair of root vertices.
    </p>
  </td>
</tr>




</tbody></table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
    <heading></heading>
  </td>
</tr>
</tbody></table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      <heading>Pending</heading>
    </td>
  </tr>
</tbody></table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
					
<tr onmouseout="crim_stop()" onmouseover="crim_start()">
  <td style="padding:20px;width:30%;height:220px;vertical-align:top">
    <div class="one">
      <div class="two" id='crim_image'>
        <img src='images/crim/crim_after.png' width="240"></div>
      <img src='images/crim/crim_before.png' width="240">
    </div>
    <script type="text/javascript">
      function crim_start() {
        document.getElementById('crim_image').style.opacity = "1";
      }

      function crim_stop() {
        document.getElementById('crim_image').style.opacity = "0";
      }
      crim_stop()
    </script>
  </td>
  <td style="padding:20px;width:70%;height:220px;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2211.12048.pdf">
      <papertitle>CRiM-GS: Continuous Rigid Motion-Aware Gaussian Splatting from Motion Blur Images</papertitle>
    </a>
    <br>
    <strong>Jungho Lee</strong>,
    <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
    <a href="https://scholar.google.com/citations?user=BaFYtwgAAAAJ&hl=ko">Donghyeong Kim</a>,
    <a href="https://dogyoonlee.github.io">Dogyoon Lee</a>,
    <a href="https://suhwan-cho.github.io">Suhwan Cho</a>,
    <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
    <br>
    <em>Submitted to NeurIPS 2024</em>
    <br>
    [
    <a href="https://Jho-Yonsei.github.io/CRiM-Gaussian">Project Page</a>
    /
    Paper
    /
    <a href="https://github.com/Jho-Yonsei/CRiM-GS">Code</a>
    /
    bib
    ]
    <p></p>
    <p>
    We propose continous motion-aware blur kernel on 3D gaussian splatting utilizing 3D rigid transformation and neural ordinary differential function to reconstruct accurate 3D scene from blurry images with real-time rendering speed.
    </p>
  </td>
</tr>
        

<tr onmouseout="smurf_stop()" onmouseover="smurf_start()">
  <td style="padding:20px;width:30%;height:220px;vertical-align:top">
    <div class="one">
      <div class="two" id='smurf_image'>
        <img src='images/smurf/smurf_after.png' width="240"></div>
      <img src='images/smurf/smurf_before.png' width="240">
    </div>
    <script type="text/javascript">
      function smurf_start() {
        document.getElementById('smurf_image').style.opacity = "1";
      }

      function smurf_stop() {
        document.getElementById('smurf_image').style.opacity = "0";
      }
      smurf_stop()
    </script>
  </td>
  <td style="padding:20px;width:70%;height:220px;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2211.12048.pdf">
      <papertitle>SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields</papertitle>
    </a>
    <br>
    <strong>Jungho Lee</strong>,
    <a href="https://dogyoonlee.github.io">Dogyoon Lee</a>,
    <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
    <a href="https://scholar.google.com/citations?user=BaFYtwgAAAAJ&hl=ko">Donghyeong Kim</a>,
    <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
    <br>
    <em>Submitted to ECCV 2024</em>
    <br>
    [
    Project Page
    /
    <a href="https://arxiv.org/pdf/2403.07547">Paper</a>
    /
    <a href="https://github.com/Jho-Yonsei/SMURF">Code</a>
    /
    <a href="data/bib/smurf2024.txt">bib</a>
    ]
    <p></p>
    <p>
    We propose novel blur kernel for motion estimation based on neural ordinary differential function to construct the deblurred neural radiance fields.
    </p>
  </td>
</tr>

<tr onmouseout="btmae_stop()" onmouseover="btmae_start()">
  <td style="padding:20px;width:30%;height:220px;vertical-align:top">
    <div class="one">
      <div class="two" id='btmae_image'>
        <img src='images/btmae/btmae_after.png' width="240"></div>
      <img src='images/btmae/btmae_before.png' width="240">
    </div>
    <script type="text/javascript">
      function btmae_start() {
        document.getElementById('btmae_image').style.opacity = "1";
      }

      function btmae_stop() {
        document.getElementById('btmae_image').style.opacity = "0";
      }
      btmae_stop()
    </script>
  </td>
  <td style="padding:20px;width:70%;height:220px;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2311.17952">
      <papertitle>Synchronizing Vision and Language: Bidirectional Token-Masking AutoEncoder for Referring Image Segmentation</papertitle>
    </a>
    <br>
    <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
    <a href="https://dogyoonlee.github.io">Dogyoon Lee</a>,
    <strong>Jungho Lee</strong>,
    <a href="https://suhwan-cho.github.io">Suhwan Cho</a>,
    Heeseung Choi,
    Ig-Jae Kim,
    <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
    <br>
    <em>Submitted to ECCV 2024</em>
    <br>
    [
    <a href="https://arxiv.org/pdf/2311.17952">Paper</a>
    /
    Code
    /
    <a href="data/bib/btmae2024.txt">bib</a>
    ]
    <p></p>
    <p>
    We propose novel bi-directional token-masking autoencoder (BTMAE) for referring image segmentation (RIS) to effectively utilize contextual information between language and visual features.
    </p>
  </td>
</tr>

<tr onmouseout="derf_stop()" onmouseover="derf_start()">
  <td style="padding:20px;width:30%;height:220px;vertical-align:top">
    <div class="one">
      <div class="two" id='derf_image'>
        <img src='images/derf/derf_after.png' width="240"></div>
      <img src='images/derf/derf_before.png' width="240">
    </div>
    <script type="text/javascript">
      function derf_start() {
        document.getElementById('derf_image').style.opacity = "1";
      }

      function derf_stop() {
        document.getElementById('derf_image').style.opacity = "0";
      }
      derf_stop()
    </script>
  </td>
  <td style="padding:20px;width:70%;height:220px;vertical-align:middle">
      <papertitle>Sparse-DeRF: Deblurred Neural Radiance Fields from Sparse View</papertitle>
    <br>
    <a href="https://dogyoonlee.github.io">Dogyoon Lee</a>,
    <a href="https://scholar.google.com/citations?user=BaFYtwgAAAAJ&hl=ko">Donghyeong Kim</a>,
    <strong>Jungho Lee</strong>,
    <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
    <a href="https://github.com/iseunghoon">Seunghoon Lee</a>,
    <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
    <br>
    <em>Submitted to TPAMI</em>, 2024
    <br>
    [
    Paper
    /
    Code
    /
    bib
    ]
    <p></p>
    <p>
    We propose enhanced deblurred neural radiance fields from sparse view settings for more practical applications considering real-world scenarios.
    </p>
  </td>
</tr>

<tr onmouseout="tmo_stop()" onmouseover="tmo_start()">
  <td style="padding:20px;width:30%;height:220px;vertical-align:top">
    <div class="one">
      <div class="two" id='tmo_image'>
        <img src='images/tmo/tmo_after.png' width="240"></div>
      <img src='images/tmo/tmo_before.png' width="240">
    </div>
    <script type="text/javascript">
      function tmo_start() {
        document.getElementById('tmo_image').style.opacity = "1";
      }

      function tmo_stop() {
        document.getElementById('tmo_image').style.opacity = "0";
      }
      tmo_stop()
    </script>
  </td>
  <td style="padding:20px;width:70%;height:220px;vertical-align:middle">
    <a href="https://arxiv.org/pdf/2309.14786">
      <papertitle>Treating Motion as Option with Output Selection for Unsupervised Video Object Segmentation</papertitle>
    </a>
    <br>
    <a href="https://suhwan-cho.github.io">Suhwan Cho</a>,
    <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
    <strong>Jungho Lee</strong>,
    <a href="https://vslab.khu.ac.kr/">MyeongAh Cho</a>,
    <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
    <br>
    <em>Submitted to Pattern Recognition (PR)</em>, 2024
    <br>
    [
    <a href="https://arxiv.org/pdf/2309.14786">Paper</a>
    /
    Code
    /
    <a href="data/bib/tmo2024.txt">bib</a>
    ]
    <p></p>
    <p>
    We propose a novel motion-as-option network by treating motion cues as optional and an adaptive output selection algorithm to adopt optimal prediction result at test time.
    </p>
  </td>
</tr>

          
        

					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website's source code is borrowed from <a href="https://github.com/jonbarron/jonbarron_website">jonbarron's website</a>.
              </p>
              <p style="text-align:right;font-size:small;">
                Last updated July 2023.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
