<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jungho Lee</title>
  
  <meta name="author" content="Jungho Lee">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="./images/logo/yonsei_logo.png">
  <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono&display=swap" rel="stylesheet">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jungho Lee</name>
              </p>
                I am a Ph.D candidate at <a href="https://www.yonsei.ac.kr">Yonsei University</a> in <a href="https://en.wikipedia.org/wiki/Seoul">Seoul</a>, where I work on computer vision and machine learning. I was a research intern at <a href="https://www.navercloudcorp.com/">NAVER Cloud</a> and participating in a Human Avatar Generation project.
              <p>
                My primary areas of research are 3D neural rendering for real-world scenarios and human avatar generation with 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF).
              </p>
              <p>
                I'm always open to collaborations or suggestions. Please feel free to contact me if you have any questions or suggestions. :)
              </p>
              <p style="text-align:center">
                <a href="mailto:2015142131@yonsei.ac.kr">Email</a> &nbsp/&nbsp
                <a href="data/resume/CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.kr/citations?user=NAj3cTcAAAAJ&hl=ko">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Jho-Yonsei/">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/jungho-lee-a7b772284/">Linkedin</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile/JunghoLee2.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile/JunghoLee2.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <span style="width: 100px; text-align: right; font-family: 'Roboto Mono';">[Nov. 2025]</span> <b><span style="color: #336FFF;">One</span> paper</b> got accepted at <b>AAAI 2026</b>! <br>
                <span style="width: 100px; text-align: right; font-family: 'Roboto Mono';">[Jun. 2025]</span> <b><span style="color: #336FFF;">One</span> paper</b> got accepted at <b>ICCV 2025</b>! <br>
                <span style="width: 100px; text-align: right; font-family: 'Roboto Mono';">[Jun. 2025]</span> Selected as an <b>Outstanding Reviewer</b> for <b>CVPR 2025</b>! <br>
                <span style="width: 100px; text-align: right; font-family: 'Roboto Mono';">[Apr. 2025]</span> <b><span style="color: #336FFF;">One</span> paper</b> got accepted at <b>TPAMI</b>! <br>
                <span style="width: 100px; text-align: right; font-family: 'Roboto Mono';">[Apr. 2025]</span> <b><span style="color: #336FFF;">One</span> paper</b> was selected as a üèÜ <b>CVPR 2025 Oral Paper</b>! <br>
                <span style="width: 100px; text-align: right; font-family: 'Roboto Mono';">[Feb. 2025]</span> <b><span style="color: #336FFF;">Two</span> papers</b> got accepted at <b>CVPR 2025</b>! <br>
                <span style="width: 100px; text-align: right; font-family: 'Roboto Mono';">[Dec. 2024]</span> <b><span style="color: #336FFF;">One</span> paper</b> got accepted at <b>AAAI 2025</b>! <br>
                <span style="width: 100px; text-align: right; font-family: 'Roboto Mono';">[Aug. 2024]</span> I will start my 2024 internship at <b>NAVER Cloud Video Team</b>! <br>
                <span style="width: 100px; text-align: right; font-family: 'Roboto Mono';">[Feb. 2024]</span> <b><span style="color: #336FFF;">One</span> paper</b> got accepted at <b>CVPR 2024</b>! <br>
                <span style="width: 100px; text-align: right; font-family: 'Roboto Mono';">[Jul. 2023]</span> <b><span style="color: #336FFF;">Two</span> papers</b> got accepted at <b>ICCV 2023</b>! <br>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <!-- <p>
                I am interested in computer vision and graphics. More specifically, I enjoy working on methods that use domain-specific knowledge of geometry, graphics, and physics, to improve computer vision systems.
              </p> -->
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
    
    <tr onmouseout="monoclue_stop()" onmouseover="monoclue_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='monoclue_image'><video  width=100% muted autoplay loop>
          <source src="images/monoclue/monoclue_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/monoclue/monoclue_before.mp4' width=100%>
        </div>
        <script type="text/javascript">
          function monoclue_start() {
            document.getElementById('monoclue_image').style.opacity = "1";
          }

          function monoclue_stop() {
            document.getElementById('monoclue_image').style.opacity = "0";
          }
          monoclue_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2511.07862">
          <papertitle>MonoCLUE : Object-Aware Clustering Enhances Monocular 3D Object Detection</papertitle>
        </a>
        <br>
        Sunghun Yang,
        <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
        <strong>Jungho Lee</strong>,
        Sangyoun Lee
        <br>
        <em>The Association for the Advancement of Artificial Intelligence (<strong>AAAI</strong>)</em>, 2026
        <br>
        <a href="https://arxiv.org/abs/2511.07862">arXiv</a>
        /
        <a href="https://github.com/SungHunYang/MonoCLUE">code</a>
        <p></p>
        <p>
          We propose MonoCLUE, which enhances monocular 3D detection by leveraging both local clustering and generalized scene memory of visual features. 
        </p>
      </td>
    </tr>
    
    <tr onmouseout="crimgs_stop()" onmouseover="crimgs_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='crimgs_image'><video  width=100% muted autoplay loop>
          <source src="images/crim/crimgs.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/crim/crimgs.png' width=100%>
        </div>
        <script type="text/javascript">
          function crimgs_start() {
            document.getElementById('crimgs_image').style.opacity = "1";
          }

          function crimgs_stop() {
            document.getElementById('crimgs_image').style.opacity = "0";
          }
          crimgs_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://Jho-Yonsei.github.io/CoMoGaussian/">
          <papertitle>CoMoGaussian: Continuous Motion-Aware Gaussian Splatting from Motion-Blurred Images</papertitle>
        </a>
        <br>
        <strong>Jungho Lee</strong>,
        Donghyeong Kim,
        <a href="https://dogyoonlee.github.io/">Dogyoon Lee</a>,
        <a href="https://suhwan-cho.github.io/">Suhwan Cho</a>,
        <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
        Wonjoon Lee,
        <a href="https://taeoh-kim.github.io">Taeoh Kim</a>,
        Dongyoon Wee,
        Sangyoun Lee
        <br>
        <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025
        <br>
        <a href="https://Jho-Yonsei.github.io/CoMoGaussian">project page</a>
        /
        <a href="https://arxiv.org/abs/2503.05332">arXiv</a>
        <p></p>
        <p>
          We propose continous motion-aware blur kernel on 3D gaussian splatting utilizing 3D rigid transformation and neural ordinary differential function to reconstruct accurate 3D scene from blurry images with real-time rendering speed.
        </p>
      </td>
    </tr>

    <tr onmouseout="derf_stop()" onmouseover="derf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='derf_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/derf/derf_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <video  width=100% height=100% muted autoplay loop>
              <source src="images/derf/derf_before.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
        </div>
        <script type="text/javascript">
          function derf_start() {
            document.getElementById('derf_image').style.opacity = "1";
          }

          function derf_stop() {
            document.getElementById('derf_image').style.opacity = "0";
          }
          sam_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://dogyoonlee.github.io/sparsederf/">
			<papertitle>Sparse-DeRF: Deblurred Neural Radiance Fields from Sparse View</papertitle>
        </a>
        <br>
        <a href="https://hydragon.co.kr">Dogyoon Lee</a>,
        Donghyeong Kim, 
        <strong>Jungho Lee</strong>,
        <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
        Seunghoon Lee,
        Sangyoun Lee
        <br>
        <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>
        <br>
        <a href="https://dogyoonlee.github.io/sparsederf/">project page</a>
        /
        <a href="https://arxiv.org/abs/2407.06613">arXiv</a>
        <p></p>
        <p>
          Sparse-DeRF effectively addresses the challenging optimization problem posed by sparse blurry imagers, while simultaneously modeling clean radiance fields. More over, it proposes a more practical research direction by considering real-world scenarios.
        </p>
      </td>
    </tr>
          
    <tr onmouseout="cocogaussian_stop()" onmouseover="cocogaussian_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='cocogaussian_image'><video  width=100% muted autoplay loop>
          <source src="images/cocogaussian/coco.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/cocogaussian/coco.png' width=100%>
        </div>
        <script type="text/javascript">
          function cocogaussian_start() {
            document.getElementById('cocogaussian_image').style.opacity = "1";
          }

          function cocogaussian_stop() {
            document.getElementById('cocogaussian_image').style.opacity = "0";
          }
          cocogaussian_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://Jho-Yonsei.github.io/CoCoGaussian">
          <papertitle>CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images</papertitle>
        </a>
        <br>
				
        <strong>Jungho Lee</strong>,
        <a href="https://suhwan-cho.github.io">Suhwan Cho</a>,
        <a href="https://taeoh-kim.github.io">Taeoh Kim</a>,
        Ho-Deok Jang,
        <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
        Geonho Cha,
        Dongyoon Wee,
        <a href="https://dogyoonlee.github.io/">Dogyoon Lee</a>,
        Sangyoun Lee
        <br>
        <em>IEEE/CVF Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
        <br>
        <a href="https://Jho-Yonsei.github.io/CoCoGaussian">project page</a>
        /
        <a href="https://arxiv.org/abs/2412.16028">arXiv</a>
        <p></p>
        <p>
        CoCoGaussian models the CoC at the 3D Gaussian level, reconstructing the precise 3D scene and enabling sharp novel view synthesis from defocused images.
        </p>
      </td>
    </tr>
            

    
    <tr onmouseout="sam_stop()" onmouseover="sam_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='sam_image'>
            <img src='images/sam/sam_output.png' width="160">
          </div>
          <img src='images/sam/sam_input.png' width="160">
        </div>
        <script type="text/javascript">
          function sam_start() {
            document.getElementById('sam_image').style.opacity = "1";
          }

          function sam_stop() {
            document.getElementById('sam_image').style.opacity = "0";
          }
          sam_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2411.14723">
			<papertitle>Effective SAM Combination for Open-Vocabulary Semantic Segmentation</papertitle>
        </a>
        <br>
        <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
        <a href="https://suhwan-cho.github.io/">Suhwan Cho</a>,
        <strong>Jungho Lee</strong>,
        Sunghun Yang, 
        Heeseung Choi,
        Ig-Jae Kim,
        Sangyoun Lee
        <br>
        <em>IEEE/CVF Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
        <br>
        project page
        /
        <a href="https://arxiv.org/abs/2411.14723">arXiv</a>
        <p></p>
        <p>
          We propose a novel one-stage open-vocabulary semantic segmentation model, which effectively combines CLIP and SAM to leverage SAM‚Äôs powerful classagnostic segmentation capabilities while maintaining efficient inference.
        </p>
      </td>
    </tr>

    <tr onmouseout="smurf_stop()" onmouseover="smurf_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='smurf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/smurf/smurf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='images/smurf/smurf.png' width="160">
        </div>
        <script type="text/javascript">
          function smurf_start() {
            document.getElementById('smurf_image').style.opacity = "1";
          }

          function smurf_stop() {
            document.getElementById('smurf_image').style.opacity = "0";
          }
          smurf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://Jho-Yonsei.github.io/SMURF">
          <papertitle>SMURF: Continuous Dynamics for Motion-Deblurring Radiance Fields</papertitle>
        </a>
        <br>
        <strong>Jungho Lee</strong>,
        <a href="https://dogyoonlee.github.io">Dogyoon Lee</a>,
        <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
        <a href="https://scholar.google.com/citations?user=BaFYtwgAAAAJ&hl=ko">Donghyung Kim</a>,
        <a href="https://scholar.google.com/citations?user=b7A10VYAAAAJ&hl=ko&oi=ao">Sangyoun Lee</a>
        <br>
        <em>IEEE/CVF Computer Vision and Pattern Recognition Workshop (<strong>CVPRW</strong>)</em>, 2025
        <br>
        <a href="https://Jho-Yonsei.github.io/SMURF">project page </a>
        /
        <a href="https://arxiv.org/abs/2403.07547">arXiv</a>
        <p></p>
        <p>
          We propose novel blur kernel for motion estimation based on neural ordinary differential function to construct the deblurred neural radiance fields.
        </p>
      </td>
    </tr>

            
            
    <tr onmouseout="vi_stop()" onmouseover="vi_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='vi_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/vi/vi_after.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <video  width=100% height=100% muted autoplay loop>
              <source src="images/vi/vi_before.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
        </div>
        <script type="text/javascript">
          function vi_start() {
            document.getElementById('vi_image').style.opacity = "1";
          }

          function vi_stop() {
            document.getElementById('vi_image').style.opacity = "0";
          }
          vi_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2408.11402/">
			<papertitle>Video Diffusion Models are Strong Video Inpainter</papertitle>
        </a>
        <br>
        <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
        <a href="https://suhwan-cho.github.io/">Suhwan Cho</a>,
        Chajin Shin,
        <strong>Jungho Lee</strong>,
        Sunghun Yang, 
        Sangyoun Lee
        <br>
        <em>The Association for the Advancement of Artificial Intelligence (<strong>AAAI</strong>)</em>, 2025
        <br>
        project page
        /
        <a href="https://arxiv.org/abs/2408.11402">arXiv</a>
        <p></p>
        <p>
          We design a First Frame Filling Video Diffusion Inpainting model inspired by the capabilities of pre-trained image-to-video diffusion models that can transform the first frame image into a highly natural video.
        </p>
      </td>
    </tr>
            
            
    <tr onmouseout="gsa_stop()" onmouseover="gsa_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='gsa_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/gsa/gsa_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <video  width=100% height=100% muted autoplay loop>
            <source src="images/gsa/gsa_before.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
        <script type="text/javascript">
          function gsa_start() {
            document.getElementById('gsa_image').style.opacity = "1";
          }

          function gsa_stop() {
            document.getElementById('gsa_image').style.opacity = "0";
          }
          gsa_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2303.08314">
          <papertitle>Guided Slot Attention for Unsupervised Video Object Segmentation</papertitle>
        </a>
        <br>
        <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
        <a href="https://suhwan-cho.github.io/">Suhwan Cho</a>,
        <a href="https://dogyoonlee.github.io/">Dogyoon Lee</a>,
        Chaewon Park,
        <strong>Jungho Lee</strong>,
        Sangyoun Lee
        <br>
        <em>IEEE/CVF Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
        <br>
        <a href="https://github.com/Hydragon516/GSANet?tab=readme-ov-file">code</a>
        /
        <a href="https://arxiv.org/abs/2303.08314">arXiv</a>
        <p></p>
        <p>
          We propose a guided slot attention network to reinforce spatial structural information and obtain better foreground‚Äìbackground separation.
        </p>
      </td>
    </tr>


            
    <tr onmouseout="hdgcn_stop()" onmouseover="hdgcn_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='hdgcn_image'>
            <img src='images/hdgcn/hdgcn_after.png' width="160">
          </div>
          <img src='images/hdgcn/hdgcn_before.png' width="160">
        </div>
        <script type="text/javascript">
          function hdgcn_start() {
            document.getElementById('hdgcn_image').style.opacity = "1";
          }

          function hdgcn_stop() {
            document.getElementById('hdgcn_image').style.opacity = "0";
          }
          hdgcn_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2208.10741">
          <papertitle>Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition</papertitle>
        </a>
        <br>
        <strong>Jungho Lee</strong>,
        <a href="https://hydragon.co.kr">Minhyeok Lee</a>,
        <a href="https://dogyoonlee.github.io">Dogyoon Lee</a>,
        Sangyoun Lee
        <br>
        <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
        <br>
        <a href="https://github.com/Jho-Yonsei/HD-GCN">code</a>
        /
        <a href="https://arxiv.org/abs/2208.10741">arXiv</a>
        <p></p>
        <p>
          We propose a hierarchically decomposed graph convolution with a novel hierarchically decomposed graph, which consider the sematic correlation between the joints and the edges of the human skeleton.
        </p>
      </td>
    </tr>

    <tr onmouseout="stc_stop()" onmouseover="stc_start()" bgcolor="#ffffd0">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='stc_image'>
            <img src='images/stcnet/stcnet_after.png' width="160">
          </div>
          <img src='images/stcnet/stcnet_before.png' width="160">
        </div>
        <script type="text/javascript">
          function stc_start() {
            document.getElementById('stc_image').style.opacity = "1";
          }

          function stc_stop() {
            document.getElementById('stc_image').style.opacity = "0";
          }
          stc_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2212.04761">
          <papertitle>Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition</papertitle>
        </a>
        <br>
				<strong>Jungho Lee</strong>,
				<a href="https://hydragon.co.kr">Minhyeok Lee</a>,
				<a href="https://suhwan-cho.github.io">Suhwan Cho</a>,
				<a href="https://sungmin-woo.github.io/">Sungmin Woo</a>,
				Sungjun Jang,
				Sangyoun Lee
        <br>
        <em>IEEE/CVF International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023
        <br>
        <a href="https://github.com/Jho-Yonsei/STC-Net">code</a>
        /
        <a href="https://arxiv.org/abs/2212.04761">arXiv</a>
        <p></p>
        <p>
          We propose a novel Spatio-Temporal Curve Network (STC-Net) for skeleton-based action recognition, which consists of spatial modules with an spatio-temporal curve (STC) module and graph convolution with dilated kernels (DK-GC)
        </p>
      </td>
    </tr>


    <tr onmouseout="sungjun_stop()" onmouseover="sungjun_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='sungjun_image'>
            <img src='images/sungjun/sungjun_after.png' width="160">
          </div>
          <img src='images/sungjun/sungjun_before.png' width="160">
        </div>
        <script type="text/javascript">
          function sungjun_start() {
            document.getElementById('sungjun_image').style.opacity = "1";
          }

          function sungjun_stop() {
            document.getElementById('sungjun_image').style.opacity = "0";
          }
          recon_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10464339&tag=1">
          <papertitle>Multi-Scale Structural Graph Convolutional Network for Skeleton-Based Action Recognition</papertitle>
        </a>
        <br>
        Sungjun Jang,
        Heansung Lee,
        Woo Jin Kim,
        <strong>Jungho Lee</strong>,
        <a href="https://sungmin-woo.github.io/">Sungmin Woo</a>,
        Sangyoun Lee
        <br>
        <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>TCSVT</strong>)</em>
        <br>
        <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10464339&tag=1">paper</a>
        <p></p>
        <p>
          We propose the multi-scale structural graph convolutional network, the common intersection graph convolution leverages the overlapped neighbor information between neighboring vertices for a given pair of root vertices.
        </p>
      </td>
    </tr>
            
<!--             
    <tr onmouseout="boundatt_stop()" onmouseover="boundatt_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='boundatt_image'><video  width=100% muted autoplay loop>
          <source src="img/boundatt.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/boundatt.png' width=100%>
        </div>
        <script type="text/javascript">
          function boundatt_start() {
            document.getElementById('boundatt_image').style.opacity = "1";
          }

          function boundatt_stop() {
            document.getElementById('boundatt_image').style.opacity = "0";
          }
          boundatt_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://boundaryattention.github.io/">
          <papertitle>Boundary Attention: Learning to Find Faint Boundaries at Any Resolution
</papertitle>
        </a>
        <br>
        <a href="">Mia G. Polansky</a>,
        <a href="">Charles Herrmann</a>,
        <a href="https://hurjunhwa.github.io/">Junhwa Hur</a>,
        <a href="https://deqings.github.io/">Deqing Sun</a>,
        <strong>Dor Verbin</strong>,
        <a href="https://www.eecs.harvard.edu/~zickler/">Todd Zickler</a>,
        <br>
        <em>arXiv</em>, 2023
        <br>
        <a href="https://boundaryattention.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2401.00935">arXiv</a>
        <p></p>
        <p>
        We design a new boundary-aware attention mechanism to quickly find boundaries at images with low SNRs. The output is similar to the <a href="#foj">field of junctions</a>, but this works ~100x faster!
        </p>
      </td>
    </tr>

            
            
    <tr onmouseout="nuvo_stop()" onmouseover="nuvo_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nuvo_image'><video  width=100% muted autoplay loop>
          <source src="img/nuvo.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/nuvo.jpg' width=100%>
        </div>
        <script type="text/javascript">
          function nuvo_start() {
            document.getElementById('nuvo_image').style.opacity = "1";
          }

          function nuvo_stop() {
            document.getElementById('nuvo_image').style.opacity = "0";
          }
          nuvo_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://pratulsrinivasan.github.io/nuvo/">
          <papertitle>Nuvo: Neural UV Mapping for Unruly 3D Representations</papertitle>
        </a>
        <br>
        <a href="https://pratulsrinivasan.github.io/">Pratul P. Srinivasan</a>,
        <a href="http://stephangarbin.com/">Stephan J. Garbin</a>,
        <strong>Dor Verbin</strong>,
        <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>
        <br>
        <em>ECCV</em>, 2024
        <br>
        <a href="https://pratulsrinivasan.github.io/nuvo/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=hmJiOSTDQZI">video</a>
        /
        <a href="http://arxiv.org/abs/2312.05283">arXiv</a>
        <p></p>
        <p>
        We use neural fields to recover editable UV mappings for challenging geometry (e.g. volumetric representations like NeRF or DreamFusion, or meshes extracted from them).
        </p>
      </td>
    </tr>

            
            
            
            
            
            

                        
            
            
            
    <tr onmouseout="zipnerf_stop()" onmouseover="zipnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='zipnerf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/zipnerf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/zipnerf.jpg' width="160">
        </div>
        <script type="text/javascript">
          function zipnerf_start() {
            document.getElementById('zipnerf_image').style.opacity = "1";
          }

          function zipnerf_stop() {
            document.getElementById('zipnerf_image').style.opacity = "0";
          }
          zipnerf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="http://jonbarron.info/zipnerf">
          <papertitle>Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields</papertitle>
        </a>
        <br>
        <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <strong>Dor Verbin</strong>,
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
        <a href="https://phogzone.com/">Peter Hedman</a>
        <br>
        <em>ICCV</em>, 2023 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Finalist)</strong></font>
        <br>
        <a href="http://jonbarron.info/zipnerf">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=xrrhynRzC8k">video</a>
        /
        <a href="https://arxiv.org/abs/2304.06706">arXiv</a>
        <p></p>
        <p>
        We combine mip-NeRF 360 and Instant NGP to reconstruct very large scenes.
        </p>
      </td>
    </tr>
    

    <tr onmouseout="nmf_stop()" onmouseover="nmf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nmf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/nmf.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/nmf.jpg' width="160">
        </div>
        <script type="text/javascript">
          function nmf_start() {
            document.getElementById('nmf_image').style.opacity = "1";
          }

          function nmf_stop() {
            document.getElementById('nmf_image').style.opacity = "0";
          }
          nmf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://half-potato.gitlab.io/posts/nmf/">
          <papertitle>Neural Microfacet Fields for Inverse Rendering</papertitle>
        </a>
        <br>
        <a href="https://half-potato.gitlab.io/">Alexander Mai</a>,
        <strong>Dor Verbin</strong>,
        <a href="https://chei.ucsd.edu/team/fkuester/">Falko Kuester</a>,
        <a href="https://people.eecs.berkeley.edu/~sfk/">Sara Fridovich-Keil</a>
        <br>
        <em>ICCV</em>, 2023
        <br>
        <a href="https://half-potato.gitlab.io/posts/nmf/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=dKJIGLrOSLk">video</a>
        /
        <a href="https://github.com/half-potato/nmf">code</a>
        /
        <a href="https://arxiv.org/abs/2303.17806">arXiv</a>
        <p></p>
        <p>
        We treat each point in space as an infinitesimal volumetric surface element. Using MC-based rendering to fit this representation to a collection of images yields accurate geometry, materials, and illumination.
        </p>
      </td>
    </tr>
    <tr onmouseout="bakedsdf_stop()" onmouseover="bakedsdf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='bakedsdf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/bakedsdf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/bakedsdf_before.jpg' width="160">
        </div>
        <script type="text/javascript">
          function bakedsdf_start() {
            document.getElementById('bakedsdf_image').style.opacity = "1";
          }

          function bakedsdf_stop() {
            document.getElementById('bakedsdf_image').style.opacity = "0";
          }
          bakedsdf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://bakedsdf.github.io/">
          <papertitle>BakedSDF: Meshing Neural SDFs for Real-Time View Synthesis</papertitle>
        </a>
        <br>
        <a href="https://lioryariv.github.io/">Lior Yariv*</a>,
        <a href="https://phogzone.com/">Peter Hedman*</a>,
        <a href="https://creiser.github.io/">Christian Reiser</a>,
        <strong>Dor Verbin</strong>,
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
        <a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
        <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>
        <br>
        <em>SIGGRAPH</em>, 2023
        <br>
        <a href="https://bakedsdf.github.io/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=fThKXZ6uDTk">video</a>
        /
        <a href="https://arxiv.org/abs/2302.14859">arXiv</a>
        <p></p>
        <p>
        We achieve real-time view synthesis by baking a high quality mesh and fine-tuning a lightweight appearance model on top.
        </p>
      </td>
    </tr>

            
    <tr onmouseout="merf_stop()" onmouseover="merf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <div class="two" id='merf_image'><video  width=100% height=100% muted autoplay loop>
          <source src="img/merf_after.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
          <img src='img/merf_before.jpg' width="160">
        </div>
        <script type="text/javascript">
          function merf_start() {
            document.getElementById('merf_image').style.opacity = "1";
          }

          function merf_stop() {
            document.getElementById('merf_image').style.opacity = "0";
          }
          merf_stop()
        </script>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://merf42.github.io/">
          <papertitle>MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes</papertitle>
        </a>
        <br>
        <a href="https://creiser.github.io/">Christian Reiser</a>,
        <a href="https://szeliski.org/RichardSzeliski.htm">Richard Szeliski</a>,
        <strong>Dor Verbin</strong>,
        <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
        <a href="https://bmild.github.io/">Ben Mildenhall</a>,
        <a href="https://www.cvlibs.net/">Andreas Geiger</a>,
        <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
        <a href="https://phogzone.com/">Peter Hedman</a>
        <br>
        <em>SIGGRAPH</em>, 2023
        <br>
        <a href="https://merf42.github.io/">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=3EACM2JAcxc">video</a>
        /
        <a href="https://arxiv.org/abs/2302.12249">arXiv</a>
        <p></p>
        <p>
        We achieve real-time view synthesis using a volumetric rendering model with a compact representation combining a low resolution 3D feature grid and high resolution 2D feature planes.
        </p>
      </td>
    </tr>


            


        <tr onmouseout="refnerf_stop()" onmouseover="refnerf_start()">
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <div class="two" id='refnerf_image'><video  width=100% height=100% muted autoplay loop>
              <source src="img/refnerf.mp4" type="video/mp4">
              Your browser does not support the video tag.
              </video></div>
              <img src='img/refnerf.jpg' width="160">
            </div>
            <script type="text/javascript">
              function refnerf_start() {
                document.getElementById('refnerf_image').style.opacity = "1";
              }

              function refnerf_stop() {
                document.getElementById('refnerf_image').style.opacity = "0";
              }
              refnerf_stop()
            </script>
          </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://dorverbin.github.io/refnerf/index.html">
                <papertitle>Ref-NeRF: Structured View-Dependent Appearance for Neural Radiance Fields</papertitle>
              </a>
              <br>
              <strong>Dor Verbin</strong>,
              <a href="https://phogzone.com/">Peter Hedman</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>, 
              <a href="https://www.eecs.harvard.edu/~zickler">Todd Zickler</a>,
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
              <br>
        <em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation, Best Student Paper Honorable Mention)</strong></font>
              <br>
              <a href="https://dorverbin.github.io/refnerf/index.html">project page</a>
        /
              <a href="https://arxiv.org/abs/2112.03907">arXiv</a>
                    /
                    <a href="https://github.com/google-research/multinerf">code</a>
        /
              <a href="https://youtu.be/qrdRH9irAlk">video</a>
              <p></p>
              <p>We modify NeRF's representation of view-dependent appearance to improve its representation of specular appearance, and recover accurate surface normals. Our method also enables view-consistent scene editing.</p>
            </td>
          </tr>
						
          <tr onmouseout="mip360_stop()" onmouseover="mip360_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mip360_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/kitchenlego_square320_crf23.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/kitchenlego_square320.jpeg' width="160">
              </div>
              <script type="text/javascript">
                function mip360_start() {
                  document.getElementById('mip360_image').style.opacity = "1";
                }

                function mip360_stop() {
                  document.getElementById('mip360_image').style.opacity = "0";
                }
                mip360_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://jonbarron.info/mipnerf360">
                <papertitle>Mip-NeRF 360: Unbounded Anti-Aliased Neural Radiance Fields</papertitle>
              </a>
              <br>
              <a href="https://jonbarron.info/">Jonathan T. Barron</a>,
              <a href="https://bmild.github.io/">Ben Mildenhall</a>,
              <strong>Dor Verbin</strong>,
              <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>,
              <a href="https://phogzone.com/">Peter Hedman</a>
              <br>
							<em>CVPR</em>, 2022 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="http://jonbarron.info/mipnerf360">project page</a>
              /
              <a href="https://arxiv.org/abs/2111.12077">arXiv</a>
              /
                <a href="https://github.com/google-research/multinerf">code</a>
                /
              <a href="https://youtu.be/zBSH-k9GbV4">video</a>
              <p></p>
              <p>We extend mip-NeRF to produce realistic results on unbounded scenes.</p>
            </td>
          </tr>
		
        
            
            				
          <tr onmouseout="foj_stop()" onmouseover="foj_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one" id="foj">
                <div class="two" id='foj_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/foj.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/foj.png' width="160">
              </div>
              <script type="text/javascript">
                function foj_start() {
                  document.getElementById('foj_image').style.opacity = "1";
                }

                function foj_stop() {
                  document.getElementById('foj_image').style.opacity = "0";
                }
                foj_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://vision.seas.harvard.edu/foj/">
                <papertitle>Field of Junctions: Extracting Boundary Structure at Low SNR</papertitle>
              </a>
              <br>
              <strong>Dor Verbin</strong>,
              <a href="https://www.eecs.harvard.edu/~zickler">Todd Zickler</a>
              <br>
							<em>ICCV</em>, 2021
              <br>
              <a href="http://vision.seas.harvard.edu/foj/">project page</a>
              /
              <a href="https://arxiv.org/abs/2011.13866">arXiv</a>
              /
                <a href="https://github.com/dorverbin/fieldofjunctions">code</a>
              /
              <a href="https://www.youtube.com/watch?v=M0VwBw_aVQA">video</a>
              <p></p>
              <p>By modeling each patch in an image as a generalized junction, our model uses concurrencies between different boundary elements such as junctions, corners, and edges, and manages to extract boundary structure from extremely noisy images where previous methods fail.</p>
            </td>
          </tr>            

            
            
						
          <tr onmouseout="uniqueness_stop()" onmouseover="uniqueness_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='uniqueness_image'><video  width=100% height=100% muted autoplay loop>
                <source src="img/uniqueness.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='img/uniqueness.png' width="160">
              </div>
              <script type="text/javascript">
                function uniqueness_start() {
                  document.getElementById('uniqueness_image').style.opacity = "1";
                }

                function uniqueness_stop() {
                  document.getElementById('uniqueness_image').style.opacity = "0";
                }
                uniqueness_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2003.08885">
                <papertitle>Unique Geometry and Texture from Corresponding Image Patches</papertitle>
              </a>
              <br>
              <strong>Dor Verbin</strong>,
              <a href="https://www.eecs.harvard.edu/~sjg/">Steven J. Gortler</a>,
              <a href="https://www.eecs.harvard.edu/~zickler">Todd Zickler</a>
              <br>
							<em>TPAMI</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2003.08885">arXiv</a>
              /
              <a href="https://ieeexplore.ieee.org/document/9435133">paper</a>
              /
              <a href="https://www.youtube.com/watch?v=sHNtMOE7q-I">video (combined with SfT)</a>                
              <p></p>
              <p>We present a simple condition for the uniqueness of a solution to the shape from texture problem. We show that in the general case four views of a cyclostationary texture satisfy this condition and are therefore sufficient to uniquely determine shape.</p>
            </td>
          </tr>            
            
            
						
          <tr onmouseout="sft_stop()" onmouseover="sft_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='sft_image'>
                <img src='img/sft_normals.jpg' width="160">
                </div>
                <img src='img/sft.jpg' width="160">
                </div>
              <script type="text/javascript">
                function sft_start() {
                  document.getElementById('sft_image').style.opacity = "1";
                }

                function sft_stop() {
                  document.getElementById('sft_image').style.opacity = "0";
                }
                sft_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="http://vision.seas.harvard.edu/sft/">
                <papertitle>Toward a Universal Model for Shape from Texture</papertitle>
              </a>
              <br>
              <strong>Dor Verbin</strong>,
              <a href="https://www.eecs.harvard.edu/~zickler">Todd Zickler</a>
              <br>
							<em>CVPR</em>, 2020
              <br>
              <a href="http://vision.seas.harvard.edu/sft/">project page</a>
              /
              <a href="http://vision.seas.harvard.edu/sft/SFT_CVPR2020.pdf">paper</a>
              /
              <a href="http://vision.seas.harvard.edu/sft/SFT_CVPR2020_supp.pdf">supplement</a>
                /
                <a href="https://github.com/dorverbin/shapefromtexture">code and data</a>
              /
              <a href="https://www.youtube.com/watch?v=SAcOQc8tGqk">video</a>
              <p></p>
              <p>We formulate the shape from texture problem as a 3-player game. This game simultaneously estimates the underlying flat texture and object shape, and it succeeds for a large variety of texture types.</p>
            </td>
          </tr>             -->
            
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                This website's source code is borrowed from <a href="http://jonbarron.info" style="text-align:right;font-size:small;">Jon Barron's website template</a>.
              </p>
              <p style="text-align:right;font-size:small;">
                Last updated December 2024.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
